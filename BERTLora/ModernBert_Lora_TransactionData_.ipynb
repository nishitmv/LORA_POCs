{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "631fb6ad-9d28-473c-8d2a-7caefd52c74a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nishi\\PythonProject\\ImageForensics\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from datasets import Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "from peft import get_peft_model, LoraConfig, TaskType\n",
    "from transformers import  Trainer, TrainingArguments\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ed1c5995-1c2f-4c57-a345-eaecfa344380",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_company_dataset(file_path, tokenizer, max_length=128, test_size=0.2):\n",
    "    \"\"\"\n",
    "    Reads a CSV file, processes it for ModernBERT Multi-Head classification,\n",
    "    and returns Train/Val datasets + dimensions for the model heads.\n",
    "    \"\"\"\n",
    "    # 1. Load Data\n",
    "    df = pd.read_csv(f\"{company}.csv\")\n",
    "\n",
    "    # 2. Label Encoding (Local to this company/dataset)\n",
    "    # If you need global consistency across companies, pass fitted encoders instead.\n",
    "    type_encoder = LabelEncoder()\n",
    "    code_encoder = LabelEncoder()\n",
    "\n",
    "    df['cc_type_id'] = type_encoder.fit_transform(df['cc_type'])\n",
    "    df['cc_code_id'] = code_encoder.fit_transform(df['cc_code'])\n",
    "\n",
    "    # Calculate dimensions for the model heads\n",
    "    num_type_labels = len(type_encoder.classes_)\n",
    "    num_code_labels = len(code_encoder.classes_)\n",
    "\n",
    "    print(f\"Dataset Loaded: {len(df)} records\")\n",
    "    print(f\"Found {num_type_labels} Transaction Types and {num_code_labels} GL Codes.\")\n",
    "\n",
    "    # 3. Stratified Train/Test Split\n",
    "    # We create a temporary 'stratify_col' to ensure both Type and Code distributions are preserved\n",
    "    df['stratify_col'] = df['cc_type'].astype(str) + \"_\" + df['cc_code'].astype(str)\n",
    "\n",
    "    train_df, val_df = train_test_split(\n",
    "        df,\n",
    "        test_size=test_size,\n",
    "        random_state=42,\n",
    "        stratify=df['stratify_col']  # Critical for rare GL codes\n",
    "    )\n",
    "\n",
    "    # Cleanup auxiliary columns\n",
    "    cols_to_keep = ['merchant_group', 'merchant_name', 'cc_type_id', 'cc_code_id']\n",
    "    train_df = train_df[cols_to_keep]\n",
    "    val_df = val_df[cols_to_keep]\n",
    "\n",
    "    # 4. Convert to Hugging Face Datasets\n",
    "    train_dataset = Dataset.from_pandas(train_df, preserve_index=False)\n",
    "    val_dataset = Dataset.from_pandas(val_df, preserve_index=False)\n",
    "\n",
    "    # 5. Tokenization Function\n",
    "    def preprocess_function(examples):\n",
    "        # Create input: \"[CLS] Merchant Group [SEP] Merchant Name [SEP]\"\n",
    "        tokenized_inputs = tokenizer(\n",
    "            text=examples[\"merchant_group\"],\n",
    "            text_pair=examples[\"merchant_name\"],\n",
    "            truncation=True,\n",
    "            max_length=max_length,\n",
    "            padding=\"max_length\"\n",
    "        )\n",
    "\n",
    "        # Map to specific arguments expected by our custom ModernBertMultiHead\n",
    "        tokenized_inputs[\"labels_type\"] = examples[\"cc_type_id\"]\n",
    "        tokenized_inputs[\"labels_code\"] = examples[\"cc_code_id\"]\n",
    "\n",
    "        return tokenized_inputs\n",
    "\n",
    "    # 6. Apply Processing\n",
    "    # We remove the text columns to leave only the tensors\n",
    "    remove_cols = train_dataset.column_names\n",
    "\n",
    "    train_dataset = train_dataset.map(preprocess_function, batched=True, remove_columns=remove_cols)\n",
    "    val_dataset = val_dataset.map(preprocess_function, batched=True, remove_columns=remove_cols)\n",
    "\n",
    "    # 7. Set Format for PyTorch\n",
    "    target_columns = [\"input_ids\", \"attention_mask\", \"labels_type\", \"labels_code\"]\n",
    "    train_dataset.set_format(type=\"torch\", columns=target_columns)\n",
    "    val_dataset.set_format(type=\"torch\", columns=target_columns)\n",
    "\n",
    "    return {\n",
    "        \"train\": train_dataset,\n",
    "        \"val\": val_dataset,\n",
    "        \"num_type_labels\": num_type_labels,\n",
    "        \"num_code_labels\": num_code_labels,\n",
    "        \"encoders\": {\"type\": type_encoder, \"code\": code_encoder}\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "01a3aad0-c520-49f1-9faa-a95438d57c5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModernBertMultiHead(nn.Module):\n",
    "    def __init__(self, model_name, num_type_labels, num_code_labels):\n",
    "        super().__init__()\n",
    "        # Load base ModernBERT model\n",
    "        self.bert = AutoModel.from_pretrained(model_name)\n",
    "        \n",
    "        self.config = self.bert.config\n",
    "\n",
    "        hidden_size = self.bert.config.hidden_size\n",
    "        print(hidden_size)\n",
    "        # Define two separate heads\n",
    "        self.type_head = nn.Linear(hidden_size, num_type_labels)\n",
    "        self.code_head = nn.Linear(hidden_size, num_code_labels)\n",
    "\n",
    "        # Dropout for regularization\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, labels_type=None, labels_code=None, **kwargs):\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "\n",
    "        # ModernBERT typically uses Mean Pooling or [CLS] (index 0)\n",
    "        # We use index 0 (CLS-equivalent) for classification\n",
    "        sequence_output = outputs.last_hidden_state\n",
    "        pooled_output = sequence_output[:, 0, :]\n",
    "        pooled_output = self.dropout(pooled_output)\n",
    "\n",
    "        # Get logits from both heads\n",
    "        logits_type = self.type_head(pooled_output)\n",
    "        logits_code = self.code_head(pooled_output)\n",
    "\n",
    "        loss = None\n",
    "        if labels_type is not None and labels_code is not None:\n",
    "            loss_fct = nn.CrossEntropyLoss()\n",
    "            loss_type = loss_fct(logits_type, labels_type)\n",
    "            loss_code = loss_fct(logits_code, labels_code)\n",
    "\n",
    "            loss = (2.0 * loss_type) + (1.0 * loss_code) \n",
    "           # loss = loss_type + loss_code  # Sum losses\n",
    "\n",
    "        return {\"loss\": loss, \"logits_type\": logits_type, \"logits_code\": logits_code}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ae473929-ed67-466f-a83e-80f4fc8d9607",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Setup\n",
    "model_id = \"answerdotai/ModernBERT-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "companies = [\"company_A\", \"company_B\", \"company_C\"]  # Your list of companies\n",
    "\n",
    "#Define LoRA Config\n",
    "# modules_to_save is CRITICAL here. It ensures your custom heads are trainable.\n",
    "peft_config = LoraConfig(\n",
    "    task_type=TaskType.FEATURE_EXTRACTION,  # Using custom model, so generic task\n",
    "    inference_mode=False,\n",
    "    r=64,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.1,\n",
    "    target_modules=[\"Wqkv\", \"Wo\", \"Wi\", \"W2\"],  # ModernBERT target modules (verify exact names)\n",
    "    modules_to_save=[\"type_head\", \"code_head\"]  # TRAIN THESE LAYERS\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17cd9dfe-bb5a-4e16-8a76-35b92a3fdadf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training adapter for: company_A\n",
      "Dataset Loaded: 10000 records\n",
      "Found 8 Transaction Types and 25 GL Codes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 8000/8000 [00:00<00:00, 30685.97 examples/s]\n",
      "Map: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2000/2000 [00:00<00:00, 28603.70 examples/s]\n",
      "Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n",
      "Loading weights: 100%|████████████████████████████████████████████████████████████████████████| 134/134 [00:00<00:00, 1020.18it/s, Materializing param=layers.21.mlp_norm.weight]\n",
      "ModernBertModel LOAD REPORT from: answerdotai/ModernBERT-base\n",
      "Key               | Status     |  | \n",
      "------------------+------------+--+-\n",
      "decoder.bias      | UNEXPECTED |  | \n",
      "head.norm.weight  | UNEXPECTED |  | \n",
      "head.dense.weight | UNEXPECTED |  | \n",
      "\n",
      "Notes:\n",
      "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "768\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='7' max='625' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  7/625 01:08 < 2:21:47, 0.07 it/s, Epoch 0.05/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for company in companies:\n",
    "    print(f\"Training adapter for: {company}\")\n",
    "\n",
    "    # A. Get Data for this Company\n",
    "    # get_company_dataset returns a formatted HF Dataset\n",
    "    # Format: \"Merchant Group: {grp} [SEP] Merchant Name: {name}\"\n",
    "    # Get the data bundle\n",
    "    data_bundle = get_company_dataset(company, tokenizer)\n",
    "\n",
    "\n",
    "    # 2. Extract components\n",
    "    train_dataset = data_bundle[\"train\"]\n",
    "    val_dataset = data_bundle[\"val\"]\n",
    "    num_type_labels = data_bundle[\"num_type_labels\"]\n",
    "    num_code_labels = data_bundle[\"num_code_labels\"]\n",
    "    base_model = ModernBertMultiHead(model_id, num_type_labels, num_code_labels)\n",
    "    model = get_peft_model(base_model, peft_config)\n",
    "    # B. Manage Adapters\n",
    "    # If it's the first run, the 'default' adapter is active.\n",
    "    # For subsequent runs, we add a new adapter.\n",
    "    adapter_name = f\"adapter_{company}\"\n",
    "\n",
    "    try:\n",
    "        model.add_adapter(adapter_name, peft_config)\n",
    "    except ValueError:\n",
    "        pass  # Adapter might already exist if resuming\n",
    "\n",
    "    model.set_adapter(adapter_name)\n",
    "\n",
    "    # C. Train\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=f\"./results/{company}\",\n",
    "        per_device_train_batch_size=64,\n",
    "        num_train_epochs=5,\n",
    "        save_strategy=\"no\",  # We save manually to be safe\n",
    "        learning_rate=1e-3,\n",
    "        remove_unused_columns=False  # Important for custom models\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "\n",
    "    metrics = trainer.evaluate(eval_dataset=val_dataset)\n",
    "    print(metrics)\n",
    "    \n",
    "    # D. Save Adapter & Heads\n",
    "    # This saves the LoRA weights AND the 'modules_to_save' (heads) to the folder\n",
    "    model.save_pretrained(f\"./final_adapters/{company}\")\n",
    "\n",
    "    # E. Cleanup to free VRAM for next company\n",
    "    # delete_adapter removes the LoRA weights from memory\n",
    "    # Note: 'modules_to_save' weights might persist in the base model state dict\n",
    "    # if not carefully handled, but delete_adapter handles the PEFT part.\n",
    "    model.delete_adapter(adapter_name)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a5e965f-9f2f-4e9f-a604-7f2bfe12420d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "from torchmetrics.classification import AUROC\n",
    "\n",
    "# 1. Get Raw Predictions from Trainer\n",
    "# predictions is a tuple: ( (logits_type, logits_code), label_ids, metrics )\n",
    "# or a dict if your model output dict was unpacked differently. \n",
    "# For custom models, it usually returns the tuple of logits.\n",
    "predictions_output = trainer.predict(val_dataset)\n",
    "\n",
    "# Unpack logits (Trainer returns them as a tuple if model returns dict/tuple)\n",
    "# Note: Check the order! It matches your forward() return or output dict order.\n",
    "# Based on my previous code: return {\"loss\": loss, \"logits_type\": ..., \"logits_code\": ...}\n",
    "# The Trainer often returns logits as a tuple: (logits_type, logits_code)\n",
    "logits_type = predictions_output.predictions[0] \n",
    "logits_code = predictions_output.predictions[1]\n",
    "\n",
    "# Unpack True Labels (Trainer stacks them in order of dataset columns)\n",
    "# If your dataset has 'labels_type' and 'labels_code', Trainer might aggregate them.\n",
    "# SAFER WAY: Extract directly from dataset to be 100% sure of order\n",
    "true_types = val_dataset['labels_type']\n",
    "true_codes = val_dataset['labels_code']\n",
    "\n",
    "# 2. Convert Logits to Class IDs\n",
    "pred_types = np.argmax(logits_type, axis=1)\n",
    "pred_codes = np.argmax(logits_code, axis=1)\n",
    "\n",
    "# 3. Generate Confusion Matrices\n",
    "cm_type = confusion_matrix(true_types, pred_types)\n",
    "cm_code = confusion_matrix(true_codes, pred_codes)\n",
    "\n",
    "# --- Visualization Function ---\n",
    "def plot_cm(cm, class_names, title):\n",
    "    fig, ax = plt.subplots(figsize=(10, 8))\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=class_names)\n",
    "    \n",
    "    # Filter for readability if you have too many classes\n",
    "    if len(class_names) > 20:\n",
    "        plt.title(f\"{title} (Showing first 20 classes)\", fontsize=14)\n",
    "        disp.plot(ax=ax, cmap='Blues', xticks_rotation=45, include_values=False) # Hide numbers if dense\n",
    "    else:\n",
    "        plt.title(title, fontsize=14)\n",
    "        disp.plot(ax=ax, cmap='Blues', xticks_rotation=45)\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "# 4. Plot\n",
    "# Get class names from your encoders\n",
    "type_classes = data_bundle[\"encoders\"][\"type\"].classes_\n",
    "code_classes = data_bundle[\"encoders\"][\"code\"].classes_\n",
    "\n",
    "print(\"Generating Confusion Matrix for Transaction Types...\")\n",
    "plot_cm(cm_type, type_classes, \"Confusion Matrix: Transaction Types\")\n",
    "\n",
    "print(\"Generating Confusion Matrix for GL Codes...\")\n",
    "plot_cm(cm_code, code_classes, \"Confusion Matrix: GL Codes\")\n",
    "\n",
    "precision_types = precision_score(true_types, pred_types, average='weighted')\n",
    "recall_types = recall_score(true_types, pred_types, average='weighted')\n",
    "f1_types = f1_score(true_types, pred_types, average='weighted')\n",
    "print(\"Precision, Recall F1 Types % % %\", precision_types, recall_types, f1_types)\n",
    "precision_codes = precision_score(true_codes, pred_codes, average='weighted')\n",
    "recall_codes = recall_score(true_codes, pred_codes, average='weighted')\n",
    "f1_codes = f1_score(true_codes, pred_codes, average='weighted')\n",
    "print(\"Precision, Recall F1 Codes % % %\", precision_codes, recall_codes, f1_codes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d77a7e48-3685-4dba-a3dc-4d601f8f38e5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
