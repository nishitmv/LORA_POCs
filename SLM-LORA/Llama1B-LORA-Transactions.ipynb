{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-07T11:24:22.722139100Z",
     "start_time": "2026-02-07T11:24:22.691368600Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from datasets import Dataset\n",
    "from peft import get_peft_model, LoraConfig, TaskType\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from transformers import AutoConfig\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "huggingface_access_token = \"\"\n",
    "\n",
    "print(device)\n"
   ],
   "id": "176c3a0c9d24f4c0",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-07T11:24:25.875648500Z",
     "start_time": "2026-02-07T11:24:25.866520200Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def get_company_dataset(company, tokenizer, max_length=64, test_size=0.2):\n",
    "    df = pd.read_csv(f\"{company}.csv\")\n",
    "\n",
    "    #  Label Encoding (Local to this company/dataset)\n",
    "    # If global consistency needed across companies, pass fitted encoders instead.\n",
    "    type_encoder = LabelEncoder()\n",
    "    code_encoder = LabelEncoder()\n",
    "\n",
    "    df['cc_type_id'] = type_encoder.fit_transform(df['cc_type'])\n",
    "    df['cc_code_id'] = code_encoder.fit_transform(df['cc_code'])\n",
    "\n",
    "    # Calculate dimensions for the model heads\n",
    "    num_type_labels = len(type_encoder.classes_)\n",
    "    num_code_labels = len(code_encoder.classes_)\n",
    "\n",
    "    print(f\"Dataset Loaded: {len(df)} records\")\n",
    "    print(f\"Found {num_type_labels} Transaction Types and {num_code_labels} GL Codes.\")\n",
    "\n",
    "    # 3. Stratified Train/Test Split\n",
    "    # We create a temporary 'stratify_col' to ensure both Type and Code distributions are preserved\n",
    "    df['stratify_col'] = df['cc_type'].astype(str) + \"_\" + df['cc_code'].astype(str)\n",
    "\n",
    "    train_df, val_df = train_test_split(\n",
    "        df,\n",
    "        test_size=test_size,\n",
    "        random_state=42,\n",
    "        stratify=df['stratify_col']  # Critical for rare GL codes\n",
    "    )\n",
    "\n",
    "    # Cleanup auxiliary columns\n",
    "    cols_to_keep = ['merchant_group', 'merchant_name', 'cc_type_id', 'cc_code_id']\n",
    "    train_df = train_df[cols_to_keep]\n",
    "    val_df = val_df[cols_to_keep]\n",
    "\n",
    "    # 4. Convert to Hugging Face Datasets\n",
    "    train_dataset = Dataset.from_pandas(train_df, preserve_index=False)\n",
    "    val_dataset = Dataset.from_pandas(val_df, preserve_index=False)\n",
    "\n",
    "    # 5. Tokenization Function\n",
    "    def preprocess_function(examples):\n",
    "        # Manual concatenation for Qwen/LLMs\n",
    "        inputs = [f\"Group: {g} | Merchant: {n}\" for g, n in zip(examples[\"merchant_group\"], examples[\"merchant_name\"])]\n",
    "\n",
    "        tokenized_inputs = tokenizer(\n",
    "            inputs,  # Single list of strings\n",
    "            truncation=True,\n",
    "            max_length=max_length,\n",
    "            padding=\"max_length\"  # Or False if using DataCollator\n",
    "        )\n",
    "\n",
    "        tokenized_inputs[\"labels_type\"] = examples[\"cc_type_id\"]\n",
    "        tokenized_inputs[\"labels_code\"] = examples[\"cc_code_id\"]\n",
    "        return tokenized_inputs\n",
    "\n",
    "    # 6. Apply Processing\n",
    "    # We remove the text columns to leave only the tensors\n",
    "    remove_cols = train_dataset.column_names\n",
    "\n",
    "    train_dataset = train_dataset.map(preprocess_function, batched=True, remove_columns=remove_cols)\n",
    "    val_dataset = val_dataset.map(preprocess_function, batched=True, remove_columns=remove_cols)\n",
    "\n",
    "    # 7. Set Format for PyTorch\n",
    "    target_columns = [\"input_ids\", \"attention_mask\", \"labels_type\", \"labels_code\"]\n",
    "    train_dataset.set_format(type=\"torch\", columns=target_columns)\n",
    "    val_dataset.set_format(type=\"torch\", columns=target_columns)\n",
    "\n",
    "    return {\n",
    "        \"train\": train_dataset,\n",
    "        \"val\": val_dataset,\n",
    "        \"num_type_labels\": num_type_labels,\n",
    "        \"num_code_labels\": num_code_labels,\n",
    "        \"encoders\": {\"type\": type_encoder, \"code\": code_encoder}\n",
    "    }\n"
   ],
   "id": "406516f4ce8600ce",
   "outputs": [],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-07T11:24:28.276420Z",
     "start_time": "2026-02-07T11:24:28.267353Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def get_company_dataset_Hard_Val(company, tokenizer, max_length=64):\n",
    "    df = pd.read_csv(f\"hard_val_{company}.csv\")\n",
    "\n",
    "    #  Label Encoding (Local to this company/dataset)\n",
    "    # If global consistency needed across companies, pass fitted encoders instead.\n",
    "    type_encoder = LabelEncoder()\n",
    "    code_encoder = LabelEncoder()\n",
    "\n",
    "    df['cc_type_id'] = type_encoder.fit_transform(df['cc_type'])\n",
    "    df['cc_code_id'] = code_encoder.fit_transform(df['cc_code'])\n",
    "\n",
    "    # Calculate dimensions for the model heads\n",
    "    num_type_labels = len(type_encoder.classes_)\n",
    "    num_code_labels = len(code_encoder.classes_)\n",
    "\n",
    "    print(f\"Dataset Loaded: {len(df)} records\")\n",
    "    print(f\"Found {num_type_labels} Transaction Types and {num_code_labels} GL Codes.\")\n",
    "\n",
    "    # 3. Stratified Train/Test Split\n",
    "    # We create a temporary 'stratify_col' to ensure both Type and Code distributions are preserved\n",
    "    df['stratify_col'] = df['cc_type'].astype(str) + \"_\" + df['cc_code'].astype(str)\n",
    "\n",
    "    # Cleanup auxiliary columns\n",
    "    cols_to_keep = ['merchant_group', 'merchant_name', 'cc_type_id', 'cc_code_id']\n",
    "\n",
    "    val_df = df[cols_to_keep]\n",
    "\n",
    "    val_dataset = Dataset.from_pandas(val_df, preserve_index=False)\n",
    "\n",
    "    # 5. Tokenization Function\n",
    "    def preprocess_function(examples):\n",
    "        # Manual concatenation for Qwen/LLMs\n",
    "        inputs = [f\"{g} | {n}\" for g, n in zip(examples[\"merchant_group\"], examples[\"merchant_name\"])]\n",
    "\n",
    "        tokenized_inputs = tokenizer(\n",
    "            inputs,  # Single list of strings\n",
    "            truncation=True,\n",
    "            max_length=max_length,\n",
    "            padding=\"max_length\"  # Or False if using DataCollator\n",
    "        )\n",
    "\n",
    "        tokenized_inputs[\"labels_type\"] = examples[\"cc_type_id\"]\n",
    "        tokenized_inputs[\"labels_code\"] = examples[\"cc_code_id\"]\n",
    "        return tokenized_inputs\n",
    "\n",
    "    # 6. Apply Processing\n",
    "    # We remove the text columns to leave only the tensors\n",
    "    remove_cols = val_dataset.column_names\n",
    "    val_dataset = val_dataset.map(preprocess_function, batched=True, remove_columns=remove_cols)\n",
    "\n",
    "    # 7. Set Format for PyTorch\n",
    "    target_columns = [\"input_ids\", \"attention_mask\", \"labels_type\", \"labels_code\"]\n",
    "    val_dataset.set_format(type=\"torch\", columns=target_columns)\n",
    "\n",
    "    return {\n",
    "        \"val\": val_dataset,\n",
    "        \"num_type_labels\": num_type_labels,\n",
    "        \"num_code_labels\": num_code_labels,\n",
    "        \"encoders\": {\"type\": type_encoder, \"code\": code_encoder}\n",
    "    }\n"
   ],
   "id": "97a38237fe2f16ed",
   "outputs": [],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-07T11:24:37.044098200Z",
     "start_time": "2026-02-07T11:24:37.035919100Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class LlamaMultiHeadClassifier(nn.Module):\n",
    "    def __init__(self, model_id, num_type_labels, num_code_labels, lora_config=None):\n",
    "        super().__init__()\n",
    "        self.config = AutoConfig.from_pretrained(model_id)\n",
    "        self.llama = AutoModel.from_pretrained(\n",
    "            model_id,\n",
    "            token=huggingface_access_token,\n",
    "            #allows the library to download and execute custom Python code found in the model's Hugging Face Hub repository,\n",
    "            #rather than using the standard code built into the transformers librar\n",
    "            trust_remote_code=False,\n",
    "            device_map=None  # CUDA,\n",
    "\n",
    "        )\n",
    "\n",
    "        print(f\"VANILLA LLAMA 1B ARCHITECTURE : \\n {self.llama}\")\n",
    "\n",
    "        if lora_config is not None:\n",
    "            self.llama = get_peft_model(self.llama, lora_config)\n",
    "            self.llama.print_trainable_parameters()\n",
    "\n",
    "        # Two separate Head for Code and Type , hiodeen size of model , 1536 for Qwen .\n",
    "\n",
    "        self.type_head = nn.Linear(self.config.hidden_size, num_type_labels)\n",
    "        self.code_head = nn.Linear(self.config.hidden_size, num_code_labels)\n",
    "\n",
    "        # changing heads added to LLAMA Data Type \n",
    "        self.type_head.to(self.llama.dtype)\n",
    "        self.code_head.to(self.llama.dtype)\n",
    "\n",
    "        self.loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "    def forward(self, input_ids, attention_mask=None,\n",
    "                labels_type=None, labels_code=None, **kwargs):\n",
    "        outputs = self.llama(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            output_hidden_states=True,\n",
    "            return_dict=True\n",
    "        )\n",
    "        # Extract Last Token Embedding (EOS token), for LLMs, use Last Hidden State of last token\n",
    "        # shape : [batch, seq_length, hidden]\n",
    "        last_hidden_state = outputs.last_hidden_state\n",
    "        #print( \"Shape of last hidden state %\",last_hidden_state.shape )\n",
    "        #Shape of last hidden state = torch.Size([32, 64, 1536])\n",
    "        # Get Embedding of last token for Classification\n",
    "        if self.config.pad_token_id is None:  # Fallback if no pad token\n",
    "            sequence_lengths = -1\n",
    "        else:\n",
    "            if input_ids is not None:  # Find last non paddign token\n",
    "                sequence_lengths = (torch.ne(input_ids, self.config.pad_token_id).sum(-1) - 1).to(\n",
    "                    last_hidden_state.device)\n",
    "            else:\n",
    "                sequence_lengths = -1\n",
    "\n",
    "        # Get the Vector for last token in the sequence using sequence lenght calced above\n",
    "        #last_hidden_state shape: (Batch_Size, Sequence_Length, Hidden_Size)\n",
    "        # last_hidden_state[0] = batch size\n",
    "        # sequence_lengths contains last token indexes for each sequence .\n",
    "        # Last token is sequence is like CLS token it has learnt about the sequence\n",
    "        pooled_output = last_hidden_state[torch.arange(last_hidden_state.shape[0]), sequence_lengths]\n",
    "\n",
    "        # Pass it to the linear layers like we do CLS Token in Transforemrs\n",
    "        logits_type = self.type_head(pooled_output)\n",
    "        logits_code = self.code_head(pooled_output)\n",
    "\n",
    "        loss = None\n",
    "\n",
    "        if labels_type is not None and labels_code is not None:\n",
    "            loss_type = self.loss_fn(logits_type, labels_type)\n",
    "            loss_code = self.loss_fn(logits_code, labels_code)\n",
    "            loss = 2 * loss_type + 1 * loss_code\n",
    "        else:\n",
    "            loss = torch.tensor(0.0, device=input_ids.device, requires_grad=True)\n",
    "\n",
    "        # Form output that can work with Huggingface trainer\n",
    "        output = {\n",
    "            \"logits\": (logits_type, logits_code)\n",
    "        }\n",
    "\n",
    "        # Add optional keys ONLY if they are not None, this is to avoid failure in Validation Run\n",
    "        if loss is not None:\n",
    "            output[\"loss\"] = loss\n",
    "\n",
    "        if getattr(outputs, \"hidden_states\", None) is not None:\n",
    "            output[\"hidden_states\"] = outputs.hidden_states\n",
    "\n",
    "        if getattr(outputs, \"attentions\", None) is not None:\n",
    "            output[\"attentions\"] = outputs.attentions\n",
    "\n",
    "        return output\n",
    "\n"
   ],
   "id": "12bd05ed355a30c1",
   "outputs": [],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-07T11:24:43.455949500Z",
     "start_time": "2026-02-07T11:24:43.447784900Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class CustomTrainer(Trainer):\n",
    "    def save_model(self, output_dir=None, _internal_call=False):\n",
    "        # Save the LoRA adapters (standard behavior)\n",
    "        # Checks if model is wrapped in PEFT\n",
    "        if output_dir is None:\n",
    "            output_dir = self.args.output_dir\n",
    "\n",
    "        # Save LoRA weights\n",
    "        self.model.llama.save_pretrained(output_dir)\n",
    "\n",
    "        # MANUALLY save your custom heads\n",
    "        torch.save(self.model.type_head.state_dict(), f\"{output_dir}/type_head.bin\")\n",
    "        torch.save(self.model.code_head.state_dict(), f\"{output_dir}/code_head.bin\")\n",
    "\n",
    "        # Save tokenizer\n",
    "        self.tokenizer.save_pretrained(output_dir)\n",
    "\n"
   ],
   "id": "32eb4fd407722c8d",
   "outputs": [],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-07T11:24:49.522794300Z",
     "start_time": "2026-02-07T11:24:47.223036500Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Training and Tokenizer Setup\n",
    "\n",
    "model_id = \"meta-llama/Llama-3.2-1B\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=False, use_fast=True,\n",
    "                                          token=huggingface_access_token)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "companies = [\"company_D\", \"company_E\", \"company_F\"]  #  list of companies\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "    #treat the Llama backbone as a feature extractor.as we have custom multihead Llama ,\n",
    "    # normal classification this would have been SEQ_CLS\n",
    "    # classification heads are external to the PEFT wrapper here\n",
    "    task_type=TaskType.FEATURE_EXTRACTION,\n",
    "    r=8,  # 16 RANk is good , LORA Mattrices will be A X R and R X B .\n",
    "    lora_alpha=16,\n",
    "    # Scales Output of Lora adapter by Alpha / Rank . ( 32/16 for us) , Makes learnt weights LOUDER compared to base model weights .\n",
    "    #Scale of 2 is good .\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n",
    "    # Q K V and Output Projections selected to train as part of adapter\n",
    "    # MLP Layers are gate_proj, up_proj, down_proj , but results in very large number of paramters to learn but also gives huge accuracy benefit .\n",
    "    lora_dropout=0.1  # Prevents overfittig whern data sizes are small like our company data case.\n",
    ")\n",
    "peft_config_global = LoraConfig(\n",
    "    #treat the Llama backbone as a feature extractor.as we have custom multihead Llama ,\n",
    "    # normal classification this would have been SEQ_CLS\n",
    "    # classification heads are external to the PEFT wrapper here\n",
    "    task_type=TaskType.FEATURE_EXTRACTION,\n",
    "    r=4,  # 16 RANk is good , LORA Mattrices will be A X R and R X B .\n",
    "    lora_alpha=16,\n",
    "    # Scales Output of Lora adapter by Alpha / Rank . ( 32/16 for us) , Makes learnt weights LOUDER compared to base model weights .\n",
    "    #Scale of 2 is good .\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n",
    "    # Q K V and Output Projections selected to train as part of adapter\n",
    "    # MLP Layers are gate_proj, up_proj, down_proj , but results in very large number of paramters to learn but also gives huge accuracy benefit .\n",
    "    lora_dropout=0.2  # Prevents overfitting when data sizes are small like our company data case.\n",
    ")\n",
    "adapter_name_global = \"adapter_global\"\n",
    "global_adapter_path = \"./final_adapters_Llama/global\""
   ],
   "id": "484ac960d78acc48",
   "outputs": [],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "start_time": "2026-02-07T11:25:10.826259200Z"
    }
   },
   "cell_type": "code",
   "source": [
    "training_modes = [\"company_adapter\", \"global_adapter\"]\n",
    "# 4. Training Loop\n",
    "for company in companies:\n",
    "    for training_mode in training_modes:\n",
    "\n",
    "        print(f\"Training adapter for: {company}\")\n",
    "\n",
    "        # Format: \"Merchant Group: {grp} [SEP] Merchant Name: {name}\"\n",
    "        data_bundle = get_company_dataset(company, tokenizer)\n",
    "        train_dataset = data_bundle[\"train\"]\n",
    "        val_dataset = data_bundle[\"val\"]\n",
    "        num_type_labels = data_bundle[\"num_type_labels\"]\n",
    "        num_code_labels = data_bundle[\"num_code_labels\"]\n",
    "\n",
    "        model = LlamaMultiHeadClassifier(\n",
    "            model_id=model_id,\n",
    "            num_type_labels=num_type_labels,\n",
    "            num_code_labels=num_code_labels,\n",
    "            lora_config=peft_config\n",
    "        )\n",
    "\n",
    "        # If it's the first run, the 'default' adapter is active.\n",
    "        # For subsequent runs, we add a new adapter.\n",
    "        adapter_name_company = f\"adapter_{company}\"\n",
    "\n",
    "        try:\n",
    "            if training_mode == \"global_adapter\":\n",
    "                if os.path.exists(global_adapter_path):\n",
    "                    print(f\"Resuming Global Adapter from: {global_adapter_path}\")\n",
    "                    # Load weights from disk\n",
    "                    model.llama.load_adapter(global_adapter_path,\n",
    "                                             adapter_name=adapter_name_global, is_trainable=True)\n",
    "                else:\n",
    "                    model.llama.add_adapter(adapter_name=adapter_name_global, peft_config=peft_config_global)\n",
    "                    model.llama.set_adapter(adapter_name_global)\n",
    "            elif training_mode == \"company_adapter\":\n",
    "                model.llama.add_adapter(adapter_name=adapter_name_company, peft_config=peft_config)\n",
    "                model.llama.set_adapter(adapter_name_company)\n",
    "        except ValueError:\n",
    "            pass  # Adapter might already exist if resuming\n",
    "\n",
    "        model.type_head.reset_parameters()\n",
    "        model.code_head.reset_parameters()\n",
    "\n",
    "        if training_mode == \"company_adapter\":\n",
    "            output_dir = f\"./results/{company}\"\n",
    "        elif training_mode == \"global_adapter\":\n",
    "            output_dir = f\"./results/global\"\n",
    "        training_args = TrainingArguments(\n",
    "            output_dir=output_dir,\n",
    "            per_device_train_batch_size=32,\n",
    "            num_train_epochs=3,\n",
    "            save_strategy=\"no\",  # We save manually to be safe\n",
    "            learning_rate=2e-4,\n",
    "            remove_unused_columns=False  # Important for custom models\n",
    "        )\n",
    "\n",
    "        custom_trainer = CustomTrainer(\n",
    "            model=model,\n",
    "            args=training_args,\n",
    "            train_dataset=train_dataset\n",
    "        )\n",
    "\n",
    "        custom_trainer.train()\n",
    "\n",
    "        metrics = custom_trainer.evaluate(eval_dataset=val_dataset)\n",
    "        print(metrics)\n",
    "\n",
    "        if training_mode == \"company_adapter\":\n",
    "            save_path = f\"./final_adapters_Llama/{company}\"\n",
    "        elif training_mode == \"global_adapter\":\n",
    "            save_path = global_adapter_path\n",
    "\n",
    "        # Save Adapter & Heads\n",
    "        # This saves the LoRA weights\n",
    "        model.llama.save_pretrained(save_path)\n",
    "        torch.save(model.type_head.state_dict(), os.path.join(save_path, \"type_head.bin\"))\n",
    "        torch.save(model.code_head.state_dict(), os.path.join(save_path, \"code_head.bin\"))\n",
    "        # Cleanup to free VRAM for next company\n",
    "        # delete_adapter removes the LoRA weights from memory\n",
    "        if training_mode == \"company_adapter\":\n",
    "          model.llama.delete_adapter(adapter_name_company)\n",
    "        elif training_mode == \"global_adapter\":\n",
    "          model.llama.delete_adapter(adapter_name_global)\n"
   ],
   "id": "3f8977e303e7d656",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training adapter for: company_D\n",
      "Dataset Loaded: 10000 records\n",
      "Found 14 Transaction Types and 26 GL Codes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 8000/8000 [00:00<00:00, 53211.34 examples/s]\n",
      "Map: 100%|██████████| 2000/2000 [00:00<00:00, 50860.08 examples/s]\n",
      "Loading weights: 100%|██████████| 146/146 [00:00<00:00, 1169.55it/s, Materializing param=norm.weight]                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VANILLA LLAMA 1B ARCHITECTURE : \n",
      " LlamaModel(\n",
      "  (embed_tokens): Embedding(128256, 2048)\n",
      "  (layers): ModuleList(\n",
      "    (0-15): 16 x LlamaDecoderLayer(\n",
      "      (self_attn): LlamaAttention(\n",
      "        (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (k_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
      "        (v_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
      "        (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      )\n",
      "      (mlp): LlamaMLP(\n",
      "        (gate_proj): Linear(in_features=2048, out_features=8192, bias=False)\n",
      "        (up_proj): Linear(in_features=2048, out_features=8192, bias=False)\n",
      "        (down_proj): Linear(in_features=8192, out_features=2048, bias=False)\n",
      "        (act_fn): SiLUActivation()\n",
      "      )\n",
      "      (input_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
      "      (post_attention_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
      "    )\n",
      "  )\n",
      "  (norm): LlamaRMSNorm((2048,), eps=1e-05)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")\n",
      "trainable params: 1,703,936 || all params: 1,237,518,336 || trainable%: 0.1377\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='347' max='750' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [347/750 02:34 < 03:00, 2.23 it/s, Epoch 1.38/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": "25d6b5b843e17a801ba6172095dc5cfc"
     }
    }
   ],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def validate_company_adapter(model, company_name, adapters_root_dir, val_dataset):\n",
    "    print(f\"\\n--- Validating: {company_name} ---\")\n",
    "    adapter_path = os.path.join(adapters_root_dir, company_name)\n",
    "\n",
    "    # A. Dynamic Head Resizing\n",
    "    # We must resize heads to match the saved checkpoint BEFORE loading weights\n",
    "    try:\n",
    "        # Load state dicts to cpu to inspect shapes\n",
    "        type_state = torch.load(os.path.join(adapter_path, \"type_head.bin\"), map_location=\"cpu\")\n",
    "        code_state = torch.load(os.path.join(adapter_path, \"code_head.bin\"), map_location=\"cpu\")\n",
    "\n",
    "        n_types = type_state['weight'].shape[0]\n",
    "        n_codes = code_state['weight'].shape[0]\n",
    "\n",
    "        # Resize the linear layers on the model\n",
    "        # We reuse the existing dtype/device\n",
    "        dtype = model.llama.dtype\n",
    "        device = model.llama.device\n",
    "\n",
    "        model.type_head = torch.nn.Linear(model.config.hidden_size, n_types).to(device=device, dtype=dtype)\n",
    "        model.code_head = torch.nn.Linear(model.config.hidden_size, n_codes).to(device=device, dtype=dtype)\n",
    "\n",
    "        # Load weights\n",
    "        model.type_head.load_state_dict(type_state)\n",
    "        model.code_head.load_state_dict(code_state)\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Skipping {company_name}: Head weights not found.\")\n",
    "        return None\n",
    "\n",
    "    # B. Load LoRA Adapter\n",
    "    try:\n",
    "        # load_adapter adds the weights.\n",
    "        # Note: Since we already wrapped in get_peft_model, this should match keys correctly.\n",
    "        model.llama.load_adapter(adapter_path, adapter_name=company_name)\n",
    "        model.llama.set_adapter(company_name)\n",
    "    except Exception as e:\n",
    "        print(f\"Skipping {company_name}: Adapter load failed. {e}\")\n",
    "        return None\n",
    "\n",
    "    # C. Inference\n",
    "    eval_trainer = Trainer(\n",
    "        model=model,\n",
    "        args=TrainingArguments(\n",
    "            output_dir=\"./temp_eval\",\n",
    "            per_device_eval_batch_size=32,\n",
    "            remove_unused_columns=False,  # Essential for custom models\n",
    "            report_to=\"none\"\n",
    "        ),\n",
    "        preprocess_logits_for_metrics=preprocess_logits_for_metrics,\n",
    "    )\n",
    "\n",
    "    # Predict\n",
    "    # pass ignore_keys to avoid gathering 'loss' which might be None\n",
    "    preds = eval_trainer.predict(val_dataset, ignore_keys=[\"loss\", \"hidden_states\", \"attentions\"])\n",
    "\n",
    "    logits_type, logits_code = preds.predictions\n",
    "\n",
    "    # D. Metrics\n",
    "    pred_types = np.argmax(logits_type, axis=1)\n",
    "    pred_codes = np.argmax(logits_code, axis=1)\n",
    "    true_types = np.array(val_dataset['labels_type'])\n",
    "    true_codes = np.array(val_dataset['labels_code'])\n",
    "\n",
    "    print(f\"\\n--- Random Sample Review for {company_name} ---\")\n",
    "    import random\n",
    "    num_samples = len(true_types)\n",
    "    random_indices = random.sample(range(num_samples), min(10, num_samples))\n",
    "    type_encoder = data_bundle[\"encoders\"][\"type\"]  # Pass data_bundle to validate_company or encoders\n",
    "    code_encoder = data_bundle[\"encoders\"][\"code\"]\n",
    "\n",
    "    for idx in random_indices:\n",
    "        # Inverse transform IDs to Strings\n",
    "        actual_type_str = type_encoder.inverse_transform([true_types[idx]])[0]\n",
    "        actual_code_str = code_encoder.inverse_transform([true_codes[idx]])[0]\n",
    "\n",
    "        pred_type_str = type_encoder.inverse_transform([pred_types[idx]])[0]\n",
    "        pred_code_str = code_encoder.inverse_transform([pred_codes[idx]])[0]\n",
    "\n",
    "        # Check correctness\n",
    "        type_match = \"✅\" if true_types[idx] == pred_types[idx] else \"❌\"\n",
    "        code_match = \"✅\" if true_codes[idx] == pred_codes[idx] else \"❌\"\n",
    "\n",
    "        print(\n",
    "            f\"[{idx}] Type: {actual_type_str} -> {pred_type_str} {type_match} | Code: {actual_code_str} -> {pred_code_str} {code_match}\")\n",
    "\n",
    "    p_type = precision_score(true_types, pred_types, average='weighted', zero_division=0)\n",
    "    r_type = recall_score(true_types, pred_types, average='weighted', zero_division=0)\n",
    "    f1_type = f1_score(true_types, pred_types, average='weighted', zero_division=0)\n",
    "\n",
    "    p_code = precision_score(true_codes, pred_codes, average='weighted', zero_division=0)\n",
    "    r_code = recall_score(true_codes, pred_codes, average='weighted', zero_division=0)\n",
    "    f1_code = f1_score(true_codes, pred_codes, average='weighted', zero_division=0)\n",
    "\n",
    "    print(\n",
    "        f\"=== Transaction Types ===\\nPrecision: {p_type:.4f} | Recall: {r_type:.4f} | F1:     {f1_type:.4f} for company : {company}\")\n",
    "    print(\n",
    "        f\"=== GL Codes ===\\nPrecision: {p_code:.4f} | Recall: {r_code:.4f} | F1: {f1_code:.4f} for company : {company}\")\n",
    "\n",
    "    acc_type = accuracy_score(true_types, pred_types)\n",
    "    acc_code = accuracy_score(true_codes, pred_codes)\n",
    "\n",
    "    print(f\"  Accuracy -> Type: {acc_type:.4f} | Code: {acc_code:.4f}\")\n",
    "\n",
    "    # E. Cleanup\n",
    "    # Delete adapter to free memory for next company\n",
    "    model.llama.delete_adapter(company_name)\n",
    "\n",
    "    return {\"company\": company_name, \"acc_type\": acc_type, \"acc_code\": acc_code}\n"
   ],
   "id": "e4bde8013fcd78f3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def validate_global_adapter(model, adapters_root_dir, val_dataset):\n",
    "    print(f\"\\n--- Validating: Global Adapter ---\")\n",
    "    adapter_path = os.path.join(adapters_root_dir, adapter_name_global)\n",
    "\n",
    "    # A. Dynamic Head Resizing\n",
    "    # We must resize heads to match the saved checkpoint BEFORE loading weights\n",
    "    try:\n",
    "        # Load state dicts to cpu to inspect shapes\n",
    "        type_state = torch.load(os.path.join(adapter_path, \"type_head.bin\"), map_location=\"cpu\")\n",
    "        code_state = torch.load(os.path.join(adapter_path, \"code_head.bin\"), map_location=\"cpu\")\n",
    "\n",
    "        n_types = type_state['weight'].shape[0]\n",
    "        n_codes = code_state['weight'].shape[0]\n",
    "\n",
    "        # Resize the linear layers on the model\n",
    "        # We reuse the existing dtype/device\n",
    "        dtype = model.llama.dtype\n",
    "        device = model.llama.device\n",
    "\n",
    "        model.type_head = torch.nn.Linear(model.config.hidden_size, n_types).to(device=device, dtype=dtype)\n",
    "        model.code_head = torch.nn.Linear(model.config.hidden_size, n_codes).to(device=device, dtype=dtype)\n",
    "\n",
    "        # Load weights\n",
    "        model.type_head.load_state_dict(type_state)\n",
    "        model.code_head.load_state_dict(code_state)\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Skipping {adapter_name_global}: Head weights not found.\")\n",
    "        return None\n",
    "\n",
    "    # B. Load LoRA Adapter\n",
    "    try:\n",
    "        # load_adapter adds the weights.\n",
    "        # Note: Since we already wrapped in get_peft_model, this should match keys correctly.\n",
    "        model.llama.load_adapter(adapter_path, adapter_name=adapter_name_global)\n",
    "        model.llama.set_adapter(adapter_name_global)\n",
    "    except Exception as e:\n",
    "        print(f\"Skipping {adapter_name_global}: Adapter load failed. {e}\")\n",
    "        return None\n",
    "\n",
    "    # C. Inference\n",
    "    eval_trainer = Trainer(\n",
    "        model=model,\n",
    "        args=TrainingArguments(\n",
    "            output_dir=\"./temp_eval\",\n",
    "            per_device_eval_batch_size=32,\n",
    "            remove_unused_columns=False,  # Essential for custom models\n",
    "            report_to=\"none\"\n",
    "        ),\n",
    "        preprocess_logits_for_metrics=preprocess_logits_for_metrics,\n",
    "    )\n",
    "\n",
    "    # Predict\n",
    "    # pass ignore_keys to avoid gathering 'loss' which might be None\n",
    "    preds = eval_trainer.predict(val_dataset, ignore_keys=[\"loss\", \"hidden_states\", \"attentions\"])\n",
    "\n",
    "    logits_type, logits_code = preds.predictions\n",
    "\n",
    "    # D. Metrics\n",
    "    pred_types = np.argmax(logits_type, axis=1)\n",
    "    pred_codes = np.argmax(logits_code, axis=1)\n",
    "    true_types = np.array(val_dataset['labels_type'])\n",
    "    true_codes = np.array(val_dataset['labels_code'])\n",
    "\n",
    "    print(f\"\\n--- Random Sample Review for {adapter_name_global} ---\")\n",
    "    import random\n",
    "    num_samples = len(true_types)\n",
    "    random_indices = random.sample(range(num_samples), min(10, num_samples))\n",
    "    type_encoder = data_bundle[\"encoders\"][\"type\"]  # Pass data_bundle to validate_company or encoders\n",
    "    code_encoder = data_bundle[\"encoders\"][\"code\"]\n",
    "\n",
    "    for idx in random_indices:\n",
    "        # Inverse transform IDs to Strings\n",
    "        actual_type_str = type_encoder.inverse_transform([true_types[idx]])[0]\n",
    "        actual_code_str = code_encoder.inverse_transform([true_codes[idx]])[0]\n",
    "\n",
    "        pred_type_str = type_encoder.inverse_transform([pred_types[idx]])[0]\n",
    "        pred_code_str = code_encoder.inverse_transform([pred_codes[idx]])[0]\n",
    "\n",
    "        # Check correctness\n",
    "        type_match = \"✅\" if true_types[idx] == pred_types[idx] else \"❌\"\n",
    "        code_match = \"✅\" if true_codes[idx] == pred_codes[idx] else \"❌\"\n",
    "\n",
    "        print(\n",
    "            f\"[{idx}] Type: {actual_type_str} -> {pred_type_str} {type_match} | Code: {actual_code_str} -> {pred_code_str} {code_match}\")\n",
    "\n",
    "    p_type = precision_score(true_types, pred_types, average='weighted', zero_division=0)\n",
    "    r_type = recall_score(true_types, pred_types, average='weighted', zero_division=0)\n",
    "    f1_type = f1_score(true_types, pred_types, average='weighted', zero_division=0)\n",
    "\n",
    "    p_code = precision_score(true_codes, pred_codes, average='weighted', zero_division=0)\n",
    "    r_code = recall_score(true_codes, pred_codes, average='weighted', zero_division=0)\n",
    "    f1_code = f1_score(true_codes, pred_codes, average='weighted', zero_division=0)\n",
    "\n",
    "    print(\n",
    "        f\"=== Transaction Types ===\\nPrecision: {p_type:.4f} | Recall: {r_type:.4f} | F1:     {f1_type:.4f} for company : {company}\")\n",
    "    print(\n",
    "        f\"=== GL Codes ===\\nPrecision: {p_code:.4f} | Recall: {r_code:.4f} | F1: {f1_code:.4f} for company : {company}\")\n",
    "\n",
    "    acc_type = accuracy_score(true_types, pred_types)\n",
    "    acc_code = accuracy_score(true_codes, pred_codes)\n",
    "\n",
    "    print(f\"  Accuracy -> Type: {acc_type:.4f} | Code: {acc_code:.4f}\")\n",
    "\n",
    "    # E. Cleanup\n",
    "    # Delete adapter to free memory for next company\n",
    "    model.llama.delete_adapter(adapter_name_global)\n",
    "\n",
    "    return {\"company\": adapter_name_global, \"acc_type\": acc_type, \"acc_code\": acc_code}\n"
   ],
   "id": "2641347bf507492d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import os\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer, Trainer, TrainingArguments, AutoModel\n",
    "from peft import get_peft_model, LoraConfig, TaskType\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "# --- 1. Global Setup (Run ONCE) ---\n",
    "model_id = \"meta-llama/Llama-3.2-1B\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=False, use_fast=True)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "\n",
    "def preprocess_logits_for_metrics(logits, labels):\n",
    "    if isinstance(logits, tuple):\n",
    "        return logits\n",
    "    return logits\n",
    "\n",
    "\n",
    "# Initialize Base Model Wrapper ONCE\n",
    "# We start with dummy head sizes; we will resize them per company later.\n",
    "print(\"Loading Base Model (this takes time)...\")\n",
    "base_model_wrapper = LlamaMultiHeadClassifier(\n",
    "    model_id=model_id,\n",
    "    num_type_labels=2,  # Dummy, will be replaced with values per company\n",
    "    num_code_labels=2  # Dummy, will be replaced with values per company\n",
    ")\n",
    "\n",
    "base_model_wrapper_global = LlamaMultiHeadClassifier(\n",
    "    model_id=model_id,\n",
    "    num_type_labels=2,  # Dummy, will be replaced with values per company\n",
    "    num_code_labels=2  # Dummy, will be replaced with values per company\n",
    ")\n",
    "\n",
    "print(f\"Llama Multihead with 2 linear heads :\\n {base_model_wrapper}\")\n",
    "\n",
    "# CRITICAL FIX 1: Resize Embeddings immediately\n",
    "# This prevents \"device-side assert\" errors if tokenizer > model\n",
    "base_model_wrapper.llama.resize_token_embeddings(len(tokenizer))\n",
    "base_model_wrapper_global.llama.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "# This adds the \"base_model.model...\" structure so load_adapter works\n",
    "base_model_wrapper.llama = get_peft_model(base_model_wrapper.llama, peft_config)\n",
    "base_model_wrapper_global.llama = get_peft_model(base_model_wrapper.llama, peft_config_global)\n",
    "\n",
    "# Move to GPU once\n",
    "base_model_wrapper.to(\"cuda\")\n",
    "base_model_wrapper_global.to(\"cuda\")\n",
    "results = []\n",
    "companies = [\"company_D\", \"company_E\", \"company_F\"]\n",
    "\n",
    "for company in companies:\n",
    "    # Get Data\n",
    "    data_bundle = get_company_dataset(company, tokenizer)\n",
    "\n",
    "    # Validate using the SHARED model instance\n",
    "    res = validate_company_adapter(base_model_wrapper, company, \"./final_adapters_Llama\", data_bundle['val'])\n",
    "    if res:\n",
    "        results.append(res)\n",
    "\n",
    "for company in companies:\n",
    "    # Get Data\n",
    "    data_bundle_global = get_company_dataset_Hard_Val(company, tokenizer)\n",
    "\n",
    "    # Validate using the SHARED model instance\n",
    "    res = validate_global_adapter(base_model_wrapper_global, company, \"./final_adapters_Llama\",\n",
    "                                  data_bundle_global['val'])\n",
    "    if res:\n",
    "        results.append(res)\n",
    "\n"
   ],
   "id": "33ecdde6110d3208",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "9e48592098e592c6",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
